<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
 <HEAD>
   <TITLE> [antlr-interest] Why does Lexer of C++ run time target eat so	much memory
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:antlr-interest%40antlr.org?Subject=Re:%20%5Bantlr-interest%5D%20Why%20does%20Lexer%20of%20C%2B%2B%20run%20time%20target%20eat%20so%0A%09much%20memory&In-Reply-To=%3C5a92ffb60812161949o162383d6t25f57cd6d2cd4bfc%40mail.gmail.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="032027.html">
   <LINK REL="Next"  HREF="032028.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[antlr-interest] Why does Lexer of C++ run time target eat so	much memory</H1>
    <B>chain one</B> 
    <A HREF="mailto:antlr-interest%40antlr.org?Subject=Re:%20%5Bantlr-interest%5D%20Why%20does%20Lexer%20of%20C%2B%2B%20run%20time%20target%20eat%20so%0A%09much%20memory&In-Reply-To=%3C5a92ffb60812161949o162383d6t25f57cd6d2cd4bfc%40mail.gmail.com%3E"
       TITLE="[antlr-interest] Why does Lexer of C++ run time target eat so	much memory">chainone at gmail.com
       </A><BR>
    <I>Tue Dec 16 19:49:40 PST 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="032027.html">[antlr-interest] Why does Lexer of C++ run time target eat so much memory
</A></li>
        <LI>Next message: <A HREF="032028.html">[antlr-interest] Looking for examples to walk parser tree or AST with C runtime
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#32029">[ date ]</a>
              <a href="thread.html#32029">[ thread ]</a>
              <a href="subject.html#32029">[ subject ]</a>
              <a href="author.html#32029">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Thanks Jim.After your patient explanation, I think I am clear about where
the problem is.

I think in the current C runtime target, a huge file leads to a huge number
of tokens need to be pre-stored in memory. That fact makes ANTLR C runtime
not suitable for huge file input due to the huge memory consuming issue.

I think the best choice is definitely to hand craft a lexer(having a token
buffer) and override the token stream. I believe that would be very useful
especially in the scenarios of input couldn't be got by once such as
socket-based input.

2008/12/17 Jim Idle &lt;<A HREF="http://www.antlr.org/mailman/listinfo/antlr-interest">jimi at temporal-wave.com</A>&gt;

&gt;<i> On Tue, 16 Dec 2008 18:28:50 -0800, chain one &lt;<A HREF="http://www.antlr.org/mailman/listinfo/antlr-interest">chainone at gmail.com</A>&gt; wrote:
</I>&gt;<i>
</I>&gt;<i> Thanks Jim,
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> Your suggestion is very helpful to me.
</I>&gt;<i> I have checked my grammar according to your advice and SKIP some useless
</I>&gt;<i> tokens.
</I>&gt;<i> After doing this, the peak memory usage decrease &#194; from 660M to 480M.
</I>&gt;<i> However 480M is still a very large number to me.
</I>&gt;<i>
</I>&gt;<i> And I found some strange behaviors of Lexer of ANTLR3 C Runtime.&#194;
</I>&gt;<i>
</I>&gt;<i> The first two rules of my grammar to be called &#194; when parsing &#194; is:
</I>&gt;<i> syntax
</I>&gt;<i> : ISO_HEADER &#194; header_sec data_sec ISO_END&#194;
</I>&gt;<i> ;
</I>&gt;<i> header_sec
</I>&gt;<i>  : HEADER &#194; file_des file_name file_schema END_SEC&#194;
</I>&gt;<i> ;
</I>&gt;<i>
</I>&gt;<i> I placed breakpoints in both functions.
</I>&gt;<i> And the first parameter of function &quot;antlr3CommonTokenStreamSourceNew&quot;
</I>&gt;<i> &#194; which I think is used to adjust the token buffer size is left to the
</I>&gt;<i> default value&#194; ANTLR3_SIZE_HINT(1025).
</I>&gt;<i> &#194; &#194;  &#194;  &#194;  &#194;  &#194;  &#194; tokens = antlr3CommonTokenStreamSourceNew
</I>&gt;<i> &#194; (ANTLR3_SIZE_HINT, TOKENSOURCE(lex));
</I>&gt;<i>
</I>&gt;<i> I believe the number of tokens will never exceed 1025 when parsing, but
</I>&gt;<i> what happened later makes me doubt about this.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> When debugging, I found the breakpoint in&#194; syntax rule( top rule) was&#194;
</I>&gt;<i> triggered&#194; right after parser started to work.
</I>&gt;<i> That's OK, because synatx() is called in the main function
</I>&gt;<i>
</I>&gt;<i> But what's strange was , the breakpoint in&#194; header_sec was not be&#194; triggered&#194;
</I>&gt;<i> until the memory kept increasing to 478M!!
</I>&gt;<i> It is believed that the Lexer kept all the tokens recognized from the large
</I>&gt;<i> input file in buffer before parser really started to work.
</I>&gt;<i> Am I right? If I am, then what does ANTLR3_SIZE_HINT mean? How to adjust
</I>&gt;<i> the buffer size in C runtime?
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> 2008/12/17 Jim Idle &lt;<A HREF="http://www.antlr.org/mailman/listinfo/antlr-interest">jimi at temporal-wave.com</A>&gt;
</I>&gt;<i>
</I>&gt;&gt;<i> On Tue, 16 Dec 2008 15:54:58 -0800, chain one &lt;<A HREF="http://www.antlr.org/mailman/listinfo/antlr-interest">chainone at gmail.com</A>&gt; wrote:
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> &gt; Still waiting for help
</I>&gt;&gt;<i> &gt; I just wanna know, if c runtime target is suitable for large input?
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Yes.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> &gt;
</I>&gt;&gt;<i> &gt; On 12/16/08, chain one &lt;<A HREF="http://www.antlr.org/mailman/listinfo/antlr-interest">chainone at gmail.com</A>&gt; wrote:
</I>&gt;&gt;<i> &gt;&gt; Hi,
</I>&gt;&gt;<i> &gt;&gt; These days I am writing a parser for a kind of data file using C++. The
</I>&gt;&gt;<i> &gt;&gt; format of the data file is simple, so the rules are simple.
</I>&gt;&gt;<i> &gt;&gt; But when I feed a about 20M-size data file to the parser, the parser
</I>&gt;&gt;<i> &gt;&gt; eats
</I>&gt;&gt;<i> &gt;&gt; almost 600M+ memory.
</I>&gt;&gt;<i> &gt;&gt; I am surprised by this result and I found most memory and time were
</I>&gt;&gt;<i> &gt;&gt; consumed
</I>&gt;&gt;<i> &gt;&gt; by the Lexer.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> There is possibly something not quite right with this then.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> However, a 20M input file is going to generate a lot of tokens and you
</I>&gt;&gt;<i> need all of tokens in order to
</I>&gt;&gt;<i> parse the input, hence you are using a lot of memory - especially if a lot
</I>&gt;&gt;<i> of your tokens are just a few characters. If all your tokens were one
</I>&gt;&gt;<i> character then you would need 20M tokens - that would be the worst case and
</I>&gt;&gt;<i> your case will be something less than this.&#194;
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> One way to reduce the number of tokens is to use the SKIP(); macro on
</I>&gt;&gt;<i> tokens that you don't need the parser to see, such as ',' or ' ' and so on.
</I>&gt;&gt;<i> Otherwise they are sitting in your token stream for no reason. Only mark
</I>&gt;&gt;<i> them as hidden and keep them in the token stream if you will need to examine
</I>&gt;&gt;<i> them later. Otherwise SKIP them.
</I>&gt;&gt;<i>
</I>&gt;<i>
</I>&gt;<i> I think you are not understanding the interaction. The lexer will run first
</I>&gt;<i> and will produce a collection of ALL the tokens in the files. ONce it has
</I>&gt;<i> done this, THEN the parser will run. Hence you have a HUGE number of tokens
</I>&gt;<i> - many many more than 1024. You don't really need to change the size hint,
</I>&gt;<i> this is just how many tokens the lexer token factory pre-allocates in one
</I>&gt;<i> chunk. Increasing it will not change anything much as only the tokens are
</I>&gt;<i> getting allocated anyway (it is just faster to allocate them in chunks like
</I>&gt;<i> this).
</I>&gt;<i>
</I>&gt;<i> Looking at what you are parsing tends to make me think you would be better
</I>&gt;<i> off with a filtering lexer and avoiding the parser altogether. Then you
</I>&gt;<i> don't need to deal with any tokens, as you can SKIP them all. Or perhaps
</I>&gt;<i> this language does not lend it self to this kind of lexer/parser interaction
</I>&gt;<i> if you cannot deal with the huge memory.
</I>&gt;<i>
</I>&gt;<i> Here is another way though and that is to hand craft a lexer that does not
</I>&gt;<i> pre-allocate tokens like that and override the token stream etc.
</I>&gt;<i>
</I>&gt;<i> Jim
</I>&gt;<i>
</I>&gt;<i>
</I>-------------- next part --------------
An HTML attachment was scrubbed...
URL: <A HREF="http://www.antlr.org/pipermail/antlr-interest/attachments/20081217/692e934d/attachment.html">http://www.antlr.org/pipermail/antlr-interest/attachments/20081217/692e934d/attachment.html</A> 
</PRE>



<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="032027.html">[antlr-interest] Why does Lexer of C++ run time target eat so much memory
</A></li>
	<LI>Next message: <A HREF="032028.html">[antlr-interest] Looking for examples to walk parser tree or AST with C runtime
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#32029">[ date ]</a>
              <a href="thread.html#32029">[ thread ]</a>
              <a href="subject.html#32029">[ subject ]</a>
              <a href="author.html#32029">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://www.antlr.org/mailman/listinfo/antlr-interest">More information about the antlr-interest
mailing list</a><br>
</body></html>
