<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
 <HEAD>
   <TITLE> [antlr-interest] Exception while trying to generate Java classes	for lexer
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:antlr-interest%40antlr.org?Subject=Re:%20%5Bantlr-interest%5D%20Exception%20while%20trying%20to%20generate%20Java%20classes%0A%09for%20lexer&In-Reply-To=%3C6c9aae640806271750q2bab1114ga1d155c29cf916a0%40mail.gmail.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="028861.html">
   <LINK REL="Next"  HREF="028862.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[antlr-interest] Exception while trying to generate Java classes	for lexer</H1>
    <B>Sumanto Biswas</B> 
    <A HREF="mailto:antlr-interest%40antlr.org?Subject=Re:%20%5Bantlr-interest%5D%20Exception%20while%20trying%20to%20generate%20Java%20classes%0A%09for%20lexer&In-Reply-To=%3C6c9aae640806271750q2bab1114ga1d155c29cf916a0%40mail.gmail.com%3E"
       TITLE="[antlr-interest] Exception while trying to generate Java classes	for lexer">sumanto at gmail.com
       </A><BR>
    <I>Fri Jun 27 17:50:45 PDT 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="028861.html">[antlr-interest] grammar to latex
</A></li>
        <LI>Next message: <A HREF="028862.html">[antlr-interest] Exception while trying to generate Java	classes for lexer
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#28852">[ date ]</a>
              <a href="thread.html#28852">[ thread ]</a>
              <a href="subject.html#28852">[ subject ]</a>
              <a href="author.html#28852">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Hi,

I have a parser-lexer combination. I used to have it together, but
separated them so that I could have better control over options and
also for better readability.

When the lexer was part of parser, it generated fine. However, I took
the lexer out of the parser, and tried to generate them separately,
the parser just generated the parser.java file. When I tried to
generate from lexer, I got the exception stack trace pasted below.

The lexer is pasted below the stack trace and also attached.

Thanks in advance,
Sumanto

================================================================
ANTLR Parser Generator  Version 3.1b1 (May 20, 2008)  1989-2008
error(100): soslLexerRules.g:0:0: syntax error: buildnfa: &lt;AST&gt;:0:0:
unexpected AST node: synpred133_soslLexerRules
error(100): soslLexerRules.g:0:0: syntax error: buildnfa:
&lt;AST&gt;:184:10: expecting EOA, found '..'
error(100): soslLexerRules.g:0:0: syntax error: buildnfa: &lt;AST&gt;:
expecting EOA, found '&lt;empty tree&gt;'
error(10):  internal error: soslLexerRules.g : java.lang.NullPointerException
org.antlr.tool.NFAFactory.build_Astar(NFAFactory.java:570)
org.antlr.tool.TreeToNFAConverter.ebnf(TreeToNFAConverter.java:1937)
org.antlr.tool.TreeToNFAConverter.element(TreeToNFAConverter.java:1261)
org.antlr.tool.TreeToNFAConverter.alternative(TreeToNFAConverter.java:1008)
org.antlr.tool.TreeToNFAConverter.block(TreeToNFAConverter.java:803)
org.antlr.tool.TreeToNFAConverter.ebnf(TreeToNFAConverter.java:1989)
org.antlr.tool.TreeToNFAConverter.element(TreeToNFAConverter.java:1261)
org.antlr.tool.TreeToNFAConverter.alternative(TreeToNFAConverter.java:1008)
org.antlr.tool.TreeToNFAConverter.block(TreeToNFAConverter.java:803)
org.antlr.tool.TreeToNFAConverter.rule(TreeToNFAConverter.java:586)
org.antlr.tool.TreeToNFAConverter.rules(TreeToNFAConverter.java:412)
org.antlr.tool.TreeToNFAConverter.grammarSpec(TreeToNFAConverter.java:366)
org.antlr.tool.TreeToNFAConverter.grammar(TreeToNFAConverter.java:159)
org.antlr.tool.Grammar.buildNFA(Grammar.java:858)
org.antlr.tool.CompositeGrammar.createNFAs(CompositeGrammar.java:362)
org.antlr.Tool.process(Tool.java:284)
org.antlr.Tool.main(Tool.java:71)
================================================================

================================================================
lexer grammar soslLexerRules;


options {
	backtrack=true;
	memoize=true;
	filter=true;
	// k=2;
	// charVocabulary='\u0000'..'\uFFFE';  // support full unicode
}

@header {
import java.util.HashMap;
}


@members {
    private boolean _scanningQuery = false;

    public boolean isScanningQuery() {
        return _scanningQuery;
    }

    public void setScanningQuery(boolean scanningQuery) {
        _scanningQuery = scanningQuery;
    }

}




EQUAL 			: '=' ;
NOT_EQUAL 		: '!=' ;
LESS_THAN 		: '&lt;' ;
LESS_THAN_OR_EQUAL 	: '&lt;=' ;
GREATER_THAN 		: '&gt;' ;
GREATER_THAN_OR_EQUAL 	: '&gt;=' ;
LPAREN 			: '(' ;
RPAREN 			: ')' ;
COLON   		: ':' ;
COMMA 			: ',' ;


LITERAL : { isScanningQuery() }? '&quot;'
    (SEARCH_CHAR
        | ' '
        | '\t'
        | '\n'
        | '\r')*
     '&quot;'
   ;

SEARCH_TERM : { isScanningQuery() }? SEARCH_CHAR2 ( SEARCH_CHAR )*
    ;

LCURLY  : '{' { setScanningQuery(true); }
    ;

RCURLY  : '}' { setScanningQuery(false); }
    ;

// ----------------------------------------------------
// Following are parser tokens
// ----------------------------------------------------
FIND 		: 	{ !isScanningQuery() }? ('f'|'F') ('i'|'I') ('n'|'N') ('d'|'D') ;
LIMIT 		 : 	{ !isScanningQuery() }? ('l'|'L') ('i'|'I') ('m'|'M')
('i'|'I') ('t'|'T') ;
IN 		 : 	{ !isScanningQuery() }? ('i'|'I') ('n'|'N') ;
CONVERTCURRENCY	 :	{ !isScanningQuery() }? 'convertCurrency' ;
AMOUNT 		 : 	{ !isScanningQuery() }? 'Amount' ;
TOLABEL 	 : 	{ !isScanningQuery() }? 'toLabel' ;
ALL		 :	{ !isScanningQuery() }? 'ALL' ;
FIELDS		 :	{ !isScanningQuery() }? 'FIELDS' ;
NAME		 :	{ !isScanningQuery() }? 'NAME' ;
EMAIL		 :	{ !isScanningQuery() }? 'EMAIL' ;
PHONE		 :	{ !isScanningQuery() }? 'PHONE' ;
RETURNING	 : 	{ !isScanningQuery() }? ('r'|'R') ('e'|'E') ('t'|'T')
('u'|'U') ('r'|'R') ('n'|'N') ('i'|'I') ('n'|'N') ('g'|'G') ;
ORDER		 :	{ !isScanningQuery() }? ('o'|'O') ('r'|'R') ('d'|'D')
('e'|'E') ('r'|'R') ;
BY		 :	{ !isScanningQuery() }? ('b'|'B') ('y'|'Y') ;
ASC		 :	{ !isScanningQuery() }? 'ASC' ;
DESC		 :	{ !isScanningQuery() }? 'DESC' ;
NULLS		 :	{ !isScanningQuery() }? 'NULLS' ;
FIRST		 :	{ !isScanningQuery() }? ('f'|'F') ('i'|'I') ('r'|'R')
('s'|'S') ('t'|'T') ;
LAST		 :	{ !isScanningQuery() }? ('l'|'L') ('a'|'A') ('s'|'S') ('t'|'T') ;
WITH		 :	{ !isScanningQuery() }? ('w'|'W') ('i'|'I') ('t'|'T') ('h'|'H') ;
DIVISION	 :	{ !isScanningQuery() }? ('d'|'D') ('i'|'I') ('v'|'V')
('i'|'I') ('s'|'S') ('i'|'I') ('o'|'O') ('n'|'N') ;
AND_SCAN	 :	{ isScanningQuery() }?  ('a'|'A') ('n'|'N') ('d'|'D') ;
OR_SCAN 	 :	{ isScanningQuery() }?  ('o'|'O') ('r'|'R') ;
NOT_SCAN	 :	{ isScanningQuery() }?  ('n'|'N') ('o'|'O') ('t'|'T') ;
AND		 :	{ !isScanningQuery() }? ('a'|'A') ('n'|'N') ('d'|'D') ;
OR 		 :	{ !isScanningQuery() }? ('o'|'O') ('r'|'R') ;
NOT		 :	{ !isScanningQuery() }? ('n'|'N') ('o'|'O') ('t'|'T') ;
WHERE		 :	{ !isScanningQuery() }? ('w'|'W') ('h'|'H') ('e'|'E')
('r'|'R') ('e'|'E') ;
LIKE		 :	{ !isScanningQuery() }? ('l'|'L') ('i'|'I') ('k'|'K') ('e'|'E') ;
NULL		 :	{ !isScanningQuery() }? ('n'|'N') ('u'|'U') ('l'|'L') ('l'|'L') ;
TRUE		 :	{ !isScanningQuery() }? ('t'|'T') ('r'|'R') ('u'|'U') ('e'|'E') ;
FALSE		 :	{ !isScanningQuery() }? ('f'|'F') ('a'|'A') ('l'|'L')
('s'|'S') ('e'|'E') ;
INCLUDES	 :	{ !isScanningQuery() }? ('i'|'I') ('n'|'N') ('c'|'C')
('l'|'L') ('u'|'U') ('d'|'D') ('e'|'E') ('s'|'S') ;
EXCLUDES	 :	{ !isScanningQuery() }? ('e'|'E') ('x'|'X') ('c'|'C')
('l'|'L') ('u'|'U') ('d'|'D') ('e'|'E') ('s'|'S') ;
YESTERDAY	 :	{ !isScanningQuery() }? 'YESTERDAY' ;
TODAY 		 :	{ !isScanningQuery() }? 'TODAY' ;
TOMORROW 	 :	{ !isScanningQuery() }? 'TOMORROW' ;
LAST_WEEK 	 :	{ !isScanningQuery() }? 'LAST_WEEK' ;
THIS_WEEK 	 :	{ !isScanningQuery() }? 'THIS_WEEK' ;
NEXT_WEEK 	 :	{ !isScanningQuery() }? 'NEXT_WEEK' ;
LAST_MONTH 	 :	{ !isScanningQuery() }? 'LAST_MONTH' ;
THIS_MONTH 	 :	{ !isScanningQuery() }? 'THIS_MONTH' ;
NEXT_MONTH 	 :	{ !isScanningQuery() }? 'NEXT_MONTH' ;
LAST_90_DAYS 	 :	{ !isScanningQuery() }? 'LAST_90_DAYS' ;
NEXT_90_DAYS 	 :	{ !isScanningQuery() }? 'NEXT_90_DAYS' ;
LAST_N_DAYS 	 :	{ !isScanningQuery() }? 'LAST_N_DAYS' ;
NEXT_N_DAYS 	 :	{ !isScanningQuery() }? 'NEXT_N_DAYS' ;
THIS_QUARTER 	 :	{ !isScanningQuery() }? 'THIS_QUARTER' ;
LAST_QUARTER 	 :	{ !isScanningQuery() }? 'LAST_QUARTER' ;
NEXT_QUARTER 	 :	{ !isScanningQuery() }? 'NEXT_QUARTER' ;
THIS_YEAR 	 :	{ !isScanningQuery() }? 'THIS_YEAR' ;
LAST_YEAR 	 :	{ !isScanningQuery() }? 'LAST_YEAR' ;
NEXT_YEAR 	 :	{ !isScanningQuery() }? 'NEXT_YEAR' ;
NEXT_N_YEARS 	 :	{ !isScanningQuery() }? 'NEXT_N_YEARS' ;
LAST_N_YEARS 	 :	{ !isScanningQuery() }? 'LAST_N_YEARS' ;
THIS_FISCAL_QUARTER 	 :	{ !isScanningQuery() }? 'THIS_FISCAL_QUARTER' ;
LAST_FISCAL_QUARTER 	 :	{ !isScanningQuery() }? 'LAST_FISCAL_QUARTER' ;
NEXT_FISCAL_QUARTER 	 :	{ !isScanningQuery() }? 'NEXT_FISCAL_QUARTER' ;
NEXT_N_QUARTERS 	 :	{ !isScanningQuery() }? 'NEXT_N_QUARTERS' ;
LAST_N_QUARTERS 	 :	{ !isScanningQuery() }? 'LAST_N_QUARTERS' ;
THIS_FISCAL_YEAR 	 :	{ !isScanningQuery() }? 'THIS_FISCAL_YEAR' ;
LAST_FISCAL_YEAR 	 :	{ !isScanningQuery() }? 'LAST_FISCAL_YEAR' ;
NEXT_FISCAL_YEAR 	 :	{ !isScanningQuery() }? 'NEXT_FISCAL_YEAR' ;
NEXT_N_FISCAL_YEARS 	 :	{ !isScanningQuery() }? 'NEXT_N_FISCAL_YEARS' ;
LAST_N_FISCAL_YEARS 	 :	{ !isScanningQuery() }? 'LAST_N_FISCAL_YEARS' ;
NEXT_N_FISCAL_QUARTERS	 :	{ !isScanningQuery() }? 'NEXT_N_FISCAL_QUARTERS' ;
LAST_N_FISCAL_QUARTERS 	 :	{ !isScanningQuery() }? 'LAST_N_FISCAL_QUARTERS' ;
// ----------------------------------------------------
// ----------------------------------------------------

WS	 :	(' '
	|	'\t'
	|	'\n'
	|	'\r')
		{ skip();   }
	;
	


ID     : { !isScanningQuery() }? ( LETTER (LETTER | '_' | DIGIT)*);

PARAM  : { isScanningQuery() }? '$' LETTER (LETTER | '_' | DIGIT)*;

NUMBER  : { !isScanningQuery() }? DIGIT+ ;


QUOTE_DATA  : '\'' ( SEARCH_CHAR )+ '\'' ;


fragment
DATETIME : DATE ('t'|'T') HOUR ':' MINUTE ':' SECOND (('z'|'Z') |
(('+' |'-') HOUR ':' MINUTE))
    ;

fragment
DATE : ( DIGIT DIGIT DIGIT DIGIT ) '-' ( '0' '1'..'9' | '1' ('0' | '1'
|<i> '2') ) '-'  ( '0' '1'..'9' | '1'..'2' DIGIT | '3' ('0' | '1') )
</I>    ;

fragment
HOUR : '0'..'1' DIGIT | '2' '0'..'3'
    ;

fragment
MINUTE : '0'..'5' DIGIT
    ;

fragment
SECOND : '0'..'5' DIGIT | '60'
    ;


POSINTEGER  : DIGIT+
    ;

fragment
DIGIT : '0'..'9'
    ;

fragment
LETTER : 'a'..'z' | 'A'..'Z'
    ;
	
// same as SEARCH_CHAR, but won't match a $
fragment
SEARCH_CHAR2 :
    ( '\\&amp;'
    | '\\|'
    | '\\!'
    | '\\{'
    | '\\}'
    | '\\['
    | '\\]'
    | '\\('
    | '\\)'
    | '\\^'
    | '\\~'
    | '\\:'
    | '\\\\'
    | '\\\''
    | '\\\&quot;' //  asd
    | '\\+'
    | '\\-'
    | '\\?'
    | '\\*'
    |
   ~( ' '              // don't allow white space
    | '\t'             // don't allow white space
    | '\n'             // don't allow white space
    | '\r'             // don't allow white space
    | '&amp;'              // reserved char per spec
    | '|'              // reserved char per spec
    | '!'              // reserved char per spec
    | '{'              // reserved char per spec
    | '}'              // reserved char per spec
    | '['              // reserved char per spec
    | ']'              // reserved char per spec
    | '('              // reserved char per spec (from 10.0)
    | ')'              // reserved char per spec (from 10.0)
    | '?'              // reserved char per spec (from 10.0)
    | '^'              // reserved char per spec
    | '~'              // reserved char per spec
    | ':'              // reserved char per spec
    | '\\'             // reserved char per spec
    | '\''             // reserved char per spec
    | '\&quot;'             // not reserved, but treating as reserved for
better parsing behavior.
    | '+'              // reserved char per spec
    | '-'              // reserved char per spec
    | '$'              // reserved for params
    ) )
    ;

// FIND_VAL : 'f' 'i' 'n' 'd'
//    ;


fragment
SEARCH_CHAR :
    ( '\\&amp;'
    | '\\|'
    | '\\!'
    | '\\{'
    | '\\}'
    | '\\['
    | '\\]'
    | '\\('
    | '\\)'
    | '\\^'
    | '\\~'
    | '\\:'
    | '\\\\'
    | '\\\''
    | '\\\&quot;'
    | '\\+'
    | '\\-'
    | '\\?'
// --    | '\\*'
    |
   ~( ' '              // don't allow white space
    | '\t'             // don't allow white space
    | '\n'             // don't allow white space
    | '\r'             // don't allow white space
    | '&amp;'              // reserved char per spec
    | '|'              // reserved char per spec
    | '!'              // reserved char per spec
    | '{'              // reserved char per spec
    | '}'              // reserved char per spec
    | '['              // reserved char per spec
    | ']'              // reserved char per spec
    | '('              // reserved char per spec (from 10.0)
    | ')'              // reserved char per spec (from 10.0)
    | '?'              // reserved char per spec (from 10.0)
    | '^'              // reserved char per spec
    | '~'              // reserved char per spec
    | ':'              // reserved char per spec
    | '\\'             // reserved char per spec
    | '\''             // reserved char per spec
    | '\&quot;'             // not reserved, but treating as reserved for
better parsing behavior.
    | '+'              // reserved char per spec
    | '-'              // reserved char per spec
    ) )
    ;
================================================================
-------------- next part --------------
A non-text attachment was scrubbed...
Name: soslLexerRules.g
Type: application/octet-stream
Size: 9674 bytes
Desc: not available
Url : <A HREF="http://www.antlr.org/pipermail/antlr-interest/attachments/20080627/a73fe018/attachment-0001.obj">http://www.antlr.org/pipermail/antlr-interest/attachments/20080627/a73fe018/attachment-0001.obj</A> 
</PRE>










<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="028861.html">[antlr-interest] grammar to latex
</A></li>
	<LI>Next message: <A HREF="028862.html">[antlr-interest] Exception while trying to generate Java	classes for lexer
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#28852">[ date ]</a>
              <a href="thread.html#28852">[ thread ]</a>
              <a href="subject.html#28852">[ subject ]</a>
              <a href="author.html#28852">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://www.antlr.org/mailman/listinfo/antlr-interest">More information about the antlr-interest
mailing list</a><br>
</body></html>
