<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
 <HEAD>
   <TITLE> [antlr-interest] Natural language parsing
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:antlr-interest%40antlr.org?Subject=Re:%20%5Bantlr-interest%5D%20Natural%20language%20parsing&In-Reply-To=%3C96AD91D781CCD511A66000B0D0D15C49043DAD8C%40EXSVR1%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="025632.html">
   <LINK REL="Next"  HREF="025634.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[antlr-interest] Natural language parsing</H1>
    <B>Stuart Watt</B> 
    <A HREF="mailto:antlr-interest%40antlr.org?Subject=Re:%20%5Bantlr-interest%5D%20Natural%20language%20parsing&In-Reply-To=%3C96AD91D781CCD511A66000B0D0D15C49043DAD8C%40EXSVR1%3E"
       TITLE="[antlr-interest] Natural language parsing">SWatt at infobal.com
       </A><BR>
    <I>Tue Jan  8 14:44:07 PST 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="025632.html">[antlr-interest] about portability of code
</A></li>
        <LI>Next message: <A HREF="025634.html">[antlr-interest] AntlWorks and CommonTokenStream derivated class
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#25633">[ date ]</a>
              <a href="thread.html#25633">[ thread ]</a>
              <a href="subject.html#25633">[ subject ]</a>
              <a href="author.html#25633">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>[On with my cognitive scientist hat!]

Grammar less important than frequency may be. Start with verbs sentences
may. 

(Yes, I know it ends up sounding like Yoda.) Human language understanding
research has not lately followed the lexical -&gt; grammatical -&gt; semantic
pipeline path for all cases. A lot depends on the purpose of the
application. Information retrieval typically ignores all grammatical
structure in favour of word frequencies, because that supports robust
search, and if you have a decent query (and a large collection) to start
with, it's enough. If you need to map semantics, or are working with small
collections, these techniques are far less useful. Frequencies are handy,
but essentially they are heuristics of likelihood of information bearing.
However, people may be cued to process words &quot;out of band&quot; using other
tricks, like SWITCHING TO UPPER CASE (which you probably read before other
words without intending to) although grammatically this may not fit. 

I've been working with an eye-tracker, to explore how people read texts, and
they generally don't follow a wholly linear pattern. Some words are skipped
entirely, and there is a tendency to latch onto significant words, and even
step back to reprocess others in a new context, if time and resources
permit. Hence, many people entirely miss grammatical errors, and are
unbelievably robust in the face of language errors of all kinds. (Robustness
is why IR looks at work frequencies so much - it is a very reliable approach
in the face of errors.) The key is to think of the reader as an information
processor, trying to do a particular task. The nature of the task, how much
time they have, the source of the text, etc., all influence the strategies
they use. 

One interesting model is the predictor-substantiator style, developed by De
Jong in FRUMP. FRUMP consists of a two components: a predictor (designed to
make guesses about what the text is saying) and a substantiator (that looks
for evidence about those guesses). The two operate cyclically with lots of
backtracking. This is kind of like a *very* general parser, except that it
was originally intended to directly construct a semantic model not a
syntactic one, and it can more or less move to any token at any point in
processing. So, most standard parsers and grammars are like
easy-to-construct versions of this framework. Easy-to-construct is good,
most predictor-substantiator NLP systems are mammoth efforts in fairly
limited domains, yet they achieve a balance between the robustness of IR and
depth-of-processing of a grammar. 

The great thing about a grammar is that (especially in combination with a
part-of-speech tagger for term classification) it can get you a long way
quickly, especially with backtracking. Certainly, if I needed to build an
NLP system to extract some sort of meaning from texts in a limited domain on
a time budget, I'd start with some kind of grammar, even if it isn't
necessarily the right thing conceptually. However, the more robustness I
needed to achieve, the more I'd have to bend its rules, and there might come
a point where I ended up with something that didn't look much like a grammar
in any traditional sense. 

All the best
Stuart

-----Original Message-----
From: Andy Tripp [mailto:<A HREF="http://www.antlr.org/mailman/listinfo/antlr-interest">antlr at jazillian.com</A>]
Sent: Tuesday, January 08, 2008 4:30 PM
To: Terence Parr
Cc: <A HREF="http://www.antlr.org/mailman/listinfo/antlr-interest">antlr-interest at antlr.org</A>
Subject: Re: [antlr-interest] Natural language parsing


Terence Parr wrote:
&gt;<i>
</I>&gt;<i> ANTLR could only handle a limited deterministic subset rather than 
</I>&gt;<i> full NLP and couldn't help in that area.  I'm just saying that 
</I>&gt;<i> grammatical structure is key to NLP.  Word freq don't cut it.  I'm 
</I>&gt;<i> paraphrasing Steven Pinker, a human language expert from some 
</I>&gt;<i> fancy-pants school back east. :)
</I>My understanding is that the grammatical structure and word frequencies 
are all intertwined, too. So when you look for the verb in &quot;Woods Eyes 
Masters&quot;, you might see that &quot;eyes&quot; is used as a verb less often than 
&quot;masters&quot; is, yet that's offset by the fact that sentences almost never 
end with a verb. And even then, if &quot;Woods&quot; doesn't turn out to be a noun 
which can perform the &quot;eyes&quot; action (as determined by word frequency), 
then we might backtrack and decide that &quot;masters&quot; is the verb after all.

All in all, NLP seems like a total crapshoot compared to parsing 
programming languages. Heck, even C++ and COBOL have SOME rules that 
come close :)

Andy
</PRE>


























<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="025632.html">[antlr-interest] about portability of code
</A></li>
	<LI>Next message: <A HREF="025634.html">[antlr-interest] AntlWorks and CommonTokenStream derivated class
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#25633">[ date ]</a>
              <a href="thread.html#25633">[ thread ]</a>
              <a href="subject.html#25633">[ subject ]</a>
              <a href="author.html#25633">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://www.antlr.org/mailman/listinfo/antlr-interest">More information about the antlr-interest
mailing list</a><br>
</body></html>
